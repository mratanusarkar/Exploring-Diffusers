{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mratanusarkar/Exploring-Diffusers/blob/sdxl-on-free-colab/project-diffusers/diffusers-ipynbs/v2/fooocus-wandb-exp-001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "558f4c22-27a6-4cb0-fc93-65226af44428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'fooocus' already exists and is not an empty directory.\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "--2023-08-30 04:25:50--  https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/669786276/620e2e64-be9f-4599-904f-18ee3811e159?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230830%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230830T042430Z&X-Amz-Expires=300&X-Amz-Signature=e71df4c036b2bc7e3c55ae0b9448f511b9f7490a7ba698b1293a352e1a2dbcf7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=669786276&response-content-disposition=attachment%3B%20filename%3Dlibtcmalloc_minimal.so.4&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-08-30 04:25:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/669786276/620e2e64-be9f-4599-904f-18ee3811e159?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230830%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230830T042430Z&X-Amz-Expires=300&X-Amz-Signature=e71df4c036b2bc7e3c55ae0b9448f511b9f7490a7ba698b1293a352e1a2dbcf7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=669786276&response-content-disposition=attachment%3B%20filename%3Dlibtcmalloc_minimal.so.4&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 373960 (365K) [application/octet-stream]\n",
            "Saving to: ‘/content/libtcmalloc_minimal.so.4’\n",
            "\n",
            "/content/libtcmallo 100%[===================>] 365.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-30 04:25:50 (18.4 MB/s) - ‘/content/libtcmalloc_minimal.so.4’ saved [373960/373960]\n",
            "\n",
            "env: LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
            "Requirement already satisfied: torchsde==0.2.5 in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: einops==0.4.1 in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: safetensors==0.3.1 in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: boltons>=20.2.1 in /usr/local/lib/python3.10/dist-packages (from torchsde==0.2.5) (23.0.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from torchsde==0.2.5) (2.0.1+cu118)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from torchsde==0.2.5) (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.* in /usr/local/lib/python3.10/dist-packages (from torchsde==0.2.5) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from torchsde==0.2.5) (1.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.64.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde==0.2.5) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde==0.2.5) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde==0.2.5) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde==0.2.5) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->torchsde==0.2.5) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->torchsde==0.2.5) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->torchsde==0.2.5) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->torchsde==0.2.5) (1.3.0)\n",
            "Requirement already satisfied: pytorch_lightning==1.9.4 in /usr/local/lib/python3.10/dist-packages (1.9.4)\n",
            "Requirement already satisfied: omegaconf==2.2.3 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: gradio==3.39.0 in /usr/local/lib/python3.10/dist-packages (3.39.0)\n",
            "Requirement already satisfied: xformers==0.0.20 in /usr/local/lib/python3.10/dist-packages (0.0.20)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pygit2==1.12.2 in /usr/local/lib/python3.10/dist-packages (1.12.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (4.64.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (1.1.1)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (4.7.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (0.9.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.2.3) (4.9.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.103.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.3.1)\n",
            "Requirement already satisfied: gradio-client>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.5.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.16.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (2.2.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.3.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (3.9.5)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (9.2.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (2.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.0.6)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (2.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (0.23.2)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0) (11.0.3)\n",
            "Requirement already satisfied: pyre-extensions==0.0.29 in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.20) (0.0.29)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (3.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (16.0.6)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pygit2==1.12.2) (1.15.1)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (0.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (0.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.9.1->pygit2==1.12.2) (2.21)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (2.6.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.39.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.39.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.39.0) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.39.0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.39.0) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.39.0) (0.27.0)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.39.0) (0.17.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.39.0) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.39.0) (3.7.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.9.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.39.0) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch_lightning==1.9.4) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio==3.39.0) (1.1.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.9)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.32)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.30.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "03e61c|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/Fooocus/models/checkpoints/sd_xl_base_1.0_0.9vae.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "31464f|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/Fooocus/models/checkpoints/sd_xl_refiner_1.0_0.9vae.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "d497b0|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "/content/fooocus\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/soumik12345/fooocus -b feat/wandb-integration\n",
        "\n",
        "!apt -y update -qq\n",
        "!wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "%env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
        "\n",
        "!pip install torchsde==0.2.5 einops==0.4.1 transformers==4.30.2 safetensors==0.3.1 accelerate==0.21.0\n",
        "!pip install pytorch_lightning==1.9.4 omegaconf==2.2.3 gradio==3.39.0 xformers==0.0.20 triton==2.0.0 pygit2==1.12.2\n",
        "!pip install --upgrade wandb\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/sd_xl_base_1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors -d /content/Fooocus/models/checkpoints -o sd_xl_base_1.0_0.9vae.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/sd_xl_refiner_1.0/resolve/main/sd_xl_refiner_1.0_0.9vae.safetensors -d /content/Fooocus/models/checkpoints -o sd_xl_refiner_1.0_0.9vae.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors -d /content/Fooocus/models/loras -o sd_xl_offset_example-lora_1.0.safetensors\n",
        "\n",
        "%cd /content/fooocus\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VGAfMkuO55kn",
        "outputId": "d1dd5d58-ba66-4da7-940b-32fbb523034a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmratanusarkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4WtIYe_55ko"
      },
      "source": [
        "Create a new project by going to [https://wandb.ai/new-project](https://wandb.ai/new-project). Then put your project name and your entity name in the `WANDB_PROJECT` and `WANDB_ENTITY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v3qmBdLh55ks",
        "outputId": "e6650636-33aa-4afd-8812-f83a21fec833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_PROJECT=aesops-fables\n",
            "env: WANDB_ENTITY=ml-colabs\n"
          ]
        }
      ],
      "source": [
        "%env WANDB_PROJECT=aesops-fables\n",
        "%env WANDB_ENTITY=ml-colabs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $WANDB_PROJECT\n",
        "!echo $WANDB_ENTITY"
      ],
      "metadata": {
        "id": "OwYp6Gut2ZoO",
        "outputId": "3672062e-b3d7-474f-d598-d72a2261ce4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aesops-fables\n",
            "ml-colabs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g4K2pdld55ku",
        "outputId": "0cc334da-3cf6-45b3-e376-984a74505ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
            "Fooocus version: 1.0.41\n",
            "Inference Engine exists.\n",
            "Inference Engine checkout finished.\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Total VRAM 15102 MB, total RAM 12983 MB\n",
            "Enabling highvram mode because your GPU has more vram than your computer has ram. If you don't want this use: --normalvram\n",
            "xformers version: 0.0.20\n",
            "2023-08-30 04:26:36.891800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on public URL: https://b24853b31162b10edc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Set vram state to: HIGH_VRAM\n",
            "Device: cuda:0 Tesla T4 : cudaMallocAsync\n",
            "Using xformers cross attention\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "model_type EPS\n",
            "adm 2816\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "Base model loaded: sd_xl_base_1.0_0.9vae.safetensors\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "model_type EPS\n",
            "adm 2560\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "Refiner model loaded: sd_xl_refiner_1.0_0.9vae.safetensors\n",
            "LoRAs loaded: [('sd_xl_offset_example-lora_1.0.safetensors', 0.5), ('None', 0.5), ('None', 0.5), ('None', 0.5), ('None', 0.5)]\n",
            "App started successful. Use the app with http://127.0.0.1:7860/ or 127.0.0.1:7860 or https://b24853b31162b10edc.gradio.live\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmratanusarkar\u001b[0m (\u001b[33mml-colabs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fooocus/wandb/run-20230830_042837-rhqmuiuk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-pond-16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/rhqmuiuk\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
            "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
            " 67% 20/30 [00:20<00:10,  1.00s/it]Refiner swapped.\n",
            " 93% 28/30 [00:29<00:02,  1.07s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
            "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
            "100% 30/30 [00:31<00:00,  1.05s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            " 67% 20/30 [00:20<00:10,  1.06s/it]Refiner swapped.\n",
            "100% 30/30 [00:31<00:00,  1.07s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpleasant-pond-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/rhqmuiuk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzNTkyMDEy/version_details/v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230830_042837-rhqmuiuk/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fooocus/wandb/run-20230830_043101-8kyv7062\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-forest-17\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/8kyv7062\u001b[0m\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
            "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
            " 67% 20/30 [00:19<00:09,  1.01it/s]Refiner swapped.\n",
            " 93% 28/30 [00:28<00:02,  1.10s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
            "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
            "100% 30/30 [00:30<00:00,  1.02s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            " 67% 20/30 [00:21<00:10,  1.04s/it]Refiner swapped.\n",
            "100% 30/30 [00:31<00:00,  1.06s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrevived-forest-17\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/8kyv7062\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzNTkyMDEy/version_details/v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230830_043101-8kyv7062/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fooocus/wandb/run-20230830_043301-rzdplsbw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmild-water-18\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/rzdplsbw\u001b[0m\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
            "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
            " 67% 20/30 [00:19<00:10,  1.01s/it]Refiner swapped.\n",
            " 93% 28/30 [00:28<00:02,  1.10s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
            "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
            "100% 30/30 [00:30<00:00,  1.01s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            " 67% 20/30 [00:21<00:10,  1.04s/it]Refiner swapped.\n",
            "100% 30/30 [00:31<00:00,  1.06s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmild-water-18\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/rzdplsbw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzNTkyMDEy/version_details/v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230830_043301-rzdplsbw/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fooocus/wandb/run-20230830_043838-iarj4j8g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwandering-disco-19\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/iarj4j8g\u001b[0m\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
            "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
            " 67% 20/30 [00:19<00:10,  1.01s/it]Refiner swapped.\n",
            " 93% 28/30 [00:28<00:02,  1.09s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
            "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
            "100% 30/30 [00:30<00:00,  1.02s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            " 67% 20/30 [00:21<00:10,  1.09s/it]Refiner swapped.\n",
            "100% 30/30 [00:32<00:00,  1.09s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwandering-disco-19\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/iarj4j8g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzNTkyMDEy/version_details/v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230830_043838-iarj4j8g/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fooocus/wandb/run-20230830_044054-3qf40zbi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeep-disco-20\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/3qf40zbi\u001b[0m\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
            "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
            " 67% 20/30 [00:20<00:10,  1.05s/it]Refiner swapped.\n",
            " 93% 28/30 [00:29<00:02,  1.13s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
            "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
            "100% 30/30 [00:31<00:00,  1.05s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            " 67% 20/30 [00:21<00:10,  1.07s/it]Refiner swapped.\n",
            "100% 30/30 [00:32<00:00,  1.08s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdeep-disco-20\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/3qf40zbi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzNTkyMDEy/version_details/v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230830_044054-3qf40zbi/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fooocus/wandb/run-20230830_044259-lloef49c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglorious-snowball-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/lloef49c\u001b[0m\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:594: UserWarning: Should have tb<=t1 but got tb=14.614643096923828 and t1=14.614643.\n",
            "  warnings.warn(f\"Should have {tb_name}<=t1 but got {tb_name}={tb} and t1={self._end}.\")\n",
            " 67% 20/30 [00:19<00:09,  1.00it/s]Refiner swapped.\n",
            " 93% 28/30 [00:28<00:02,  1.10s/it]/usr/local/lib/python3.10/dist-packages/torchsde/_brownian/brownian_interval.py:585: UserWarning: Should have ta>=t0 but got ta=0.02916753850877285 and t0=0.029168.\n",
            "  warnings.warn(f\"Should have ta>=t0 but got ta={ta} and t0={self._start}.\")\n",
            "100% 30/30 [00:30<00:00,  1.02s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            " 67% 20/30 [00:21<00:10,  1.04s/it]Refiner swapped.\n",
            "100% 30/30 [00:32<00:00,  1.07s/it]\n",
            "Image generated with private log at: /content/fooocus/outputs/2023-08-30/log.html\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mglorious-snowball-21\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/runs/lloef49c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/ml-colabs/aesops-fables/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzNTkyMDEy/version_details/v3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230830_044259-lloef49c/logs\u001b[0m\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2133, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fooocus/launch.py\", line 88, in <module>\n",
            "    from webui import *\n",
            "  File \"/content/fooocus/webui.py\", line 197, in <module>\n",
            "    shared.gradio_root.launch(inbrowser=True, server_name=args.listen, server_port=args.port, share=args.share)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2049, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2135, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b24853b31162b10edc.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python launch.py --share"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# temp draft space\n",
        "\n",
        "# https://read.gov/aesop/005.html\n",
        "# a fox sitting down and looking at the grapes in disgust hanging from a high branch on a tall grape tree\n",
        "# a fox sitting down on the ground in disgust and looking at the grapes hanging from a high branch on a tall grape tree\n",
        "# a fox walking away very scornfully from a giant grape tree"
      ],
      "metadata": {
        "id": "-sES-RcaDZxB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Fox & the Grapes\n",
        "\n",
        "A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox's mouth watered as he gazed longingly at them.\n",
        "\n",
        "The bunch hung from a high branch, and the Fox had to jump for it. The first time he jumped he missed it by a long way. So he walked off a short distance and took a running leap at it, only to fall short once more. Again and again he tried, but in vain.\n",
        "\n",
        "Now he sat down and looked at the grapes in disgust.\n",
        "\n",
        "\"What a fool I am,\" he said. \"Here I am wearing myself out to get a bunch of sour grapes that are not worth gaping for.\"\n",
        "\n",
        "And off he walked very, very scornfully.\n",
        "\n",
        "*There are many who pretend to despise and belittle that which is beyond their reach.*\n",
        "\n",
        "[source](https://read.gov/aesop/005.html)"
      ],
      "metadata": {
        "id": "AV4CncLJsn_E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PIjixd0m3KR9"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}