{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOjzZY1lwDCw"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/diffusers/sdxl-text-to-image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG-1q7AywDCy"
      },
      "source": [
        "# Stable-Diffusion XL 1.0 using ðŸ¤— Diffusers\n",
        "\n",
        "This notebook demonstrates the following:\n",
        "- Performing text-conditional image-generations using [ðŸ¤— Diffusers](https://huggingface.co/docs/diffusers).\n",
        "- Using the Stable Diffusion XL Refiner pipeline to further refine the outputs of the base model.\n",
        "- Manage image generation experiments using [Weights & Biases](http://wandb.ai/geekyrakshit).\n",
        "- Log the prompts and generated images to [Weigts & Biases](http://wandb.ai/geekyrakshit) for visalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRTubA8kwDCz"
      },
      "source": [
        "## Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EzrrjZDcwDCz"
      },
      "outputs": [],
      "source": [
        "# !pip install -qq diffusers[\"torch\"] transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bESfEu2awDC0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "from diffusers import DiffusionPipeline, EulerDiscreteScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTU51SO6wDC0"
      },
      "source": [
        "## Experiment Management using Weights & Biases\n",
        "\n",
        "Managing our image generation experiments is crucial for the sake of reproducibility. Hence we sync all the configs of our experiments with our Weights & Biases run. This stores all the configs of the experiments, right from the prompts to the refinement technque and the configuration of the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eLKWVQAbwDC2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmratanusarkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/tsa2cob/Workspace/project-diffusers/diffusers-ipynbs/wandb/run-20230809_113116-sx4tq43v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mratanusarkar/stable-diffusion-xl/runs/sx4tq43v' target=\"_blank\">flowing-serenity-16</a></strong> to <a href='https://wandb.ai/mratanusarkar/stable-diffusion-xl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mratanusarkar/stable-diffusion-xl' target=\"_blank\">https://wandb.ai/mratanusarkar/stable-diffusion-xl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mratanusarkar/stable-diffusion-xl/runs/sx4tq43v' target=\"_blank\">https://wandb.ai/mratanusarkar/stable-diffusion-xl/runs/sx4tq43v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# initialize a wandb run\n",
        "wandb.init(project=\"stable-diffusion-xl\", entity=\"mratanusarkar\", job_type=\"text-to-image\", save_code=True)\n",
        "\n",
        "# define experiment configs\n",
        "config = wandb.config\n",
        "config.stable_diffusion_checkpoint = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "config.refiner_checkpoint = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "config.offload_to_cpu = True\n",
        "config.compile_model = False\n",
        "config.prompt_1 = \"A phtoto of a muscular lady, wearing a helmet and a latex suit. She is riding a futuristic bike with bright headlights. The background is of a brightly lit city with neon lights and traffic headlights causing motion blur, cyberpunk aesthetic, realistic, 8k\"\n",
        "config.prompt_2 = \"\" # Leave blank if you want both text encoders to use the same prompt\n",
        "config.negative_prompt_1 = \"paiting, nature, static, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, standing still, posing\"\n",
        "config.negative_prompt_2 = \"\"\n",
        "config.seed = 42\n",
        "config.use_ensemble_of_experts = True\n",
        "config.num_inference_steps = 100\n",
        "config.num_refinement_steps = 100\n",
        "config.high_noise_fraction = 0.8 # Set explicitly only if config.use_ensemble_of_experts is True\n",
        "config.scheduler_kwargs = {\n",
        "    \"beta_end\": 0.012,\n",
        "    \"beta_schedule\": \"scaled_linear\", # one of [\"linear\", \"scaled_linear\"]\n",
        "    \"beta_start\": 0.00085,\n",
        "    \"interpolation_type\": \"linear\", # one of [\"linear\", \"log_linear\"]\n",
        "    \"num_train_timesteps\": 1000,\n",
        "    \"prediction_type\": \"epsilon\", # one of [\"epsilon\", \"sample\", \"v_prediction\"]\n",
        "    \"steps_offset\": 1,\n",
        "    \"timestep_spacing\": \"leading\", # one of [\"linspace\", \"leading\"]\n",
        "    \"trained_betas\": None,\n",
        "    \"use_karras_sigmas\": False,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnHm0VlMwDC2"
      },
      "source": [
        "We can make the experiment deterministic based on the seed specified in the experiment configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x9ZCRmkTwDC3"
      },
      "outputs": [],
      "source": [
        "if config.seed is not None:\n",
        "    generator = [torch.Generator(device=\"cuda\").manual_seed(config.seed)]\n",
        "else:\n",
        "    generator = [torch.Generator(device=\"cuda\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCuT1GAlwDC3"
      },
      "source": [
        "## Creating the Diffusion Pipelines\n",
        "\n",
        "For performing text-conditional image generation, we use the `diffusers` library to define the diffusion pipelines corresponding to the base SDXL model and the SDXL refinement model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NtGLsMt7wDC3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a4709ac88aa476cb20d9102430089c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define base model\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    config.stable_diffusion_checkpoint,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        "    scheduler=EulerDiscreteScheduler(**config.scheduler_kwargs),\n",
        ")\n",
        "\n",
        "# Offload to CPU in case of OOM\n",
        "if config.offload_to_cpu:\n",
        "    pipe.enable_model_cpu_offload()\n",
        "else:\n",
        "    pipe.to(\"cuda\")\n",
        "\n",
        "# Compile model using `torch.compile`, this might give a significant speedup\n",
        "if config.compile_model:\n",
        "    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Z3u5ss7wDC3"
      },
      "outputs": [],
      "source": [
        "# # Define base model\n",
        "# refiner = DiffusionPipeline.from_pretrained(\n",
        "#     config.refiner_checkpoint,\n",
        "#     text_encoder_2=pipe.text_encoder_2,\n",
        "#     vae=pipe.vae,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     use_safetensors=True,\n",
        "#     variant=\"fp16\",\n",
        "#     scheduler=EulerDiscreteScheduler(**config.scheduler_kwargs),\n",
        "# )\n",
        "\n",
        "# # Offload to CPU in case of OOM\n",
        "# if config.offload_to_cpu:\n",
        "#     refiner.enable_model_cpu_offload()\n",
        "# else:\n",
        "#     refiner.to(\"cuda\")\n",
        "\n",
        "# # Compile model using `torch.compile`, this might give a significant speedup\n",
        "# if config.compile_model:\n",
        "#     refiner.unet = torch.compile(refiner.unet, mode=\"reduce-overhead\", fullgraph=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y3SaU2KwDC4"
      },
      "source": [
        "We now define a utility function to postprocess the latents obtained from the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xoW8vRbTwDC4"
      },
      "outputs": [],
      "source": [
        "def postprocess_latent(latent):\n",
        "    vae_output = pipe.vae.decode(\n",
        "        latent.images / pipe.vae.config.scaling_factor, return_dict=False\n",
        "    )[0].detach()\n",
        "    return pipe.image_processor.postprocess(vae_output, output_type=\"pil\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acW8AauPwDC4"
      },
      "source": [
        "## Text-to-Image Generation\n",
        "\n",
        "Now, we pass the prompts and the negative prompts to the base model and then pass the output to the refiner for firther refinement. In order to know more about the different refinement techniques that can be used with SDXL, you can check [`diffusers` docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kep2tgbpwDC4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e248c00474b74f609a7960c0e83cb14e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 23.68 GiB total capacity; 6.27 GiB already allocated; 264.38 MiB free; 6.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     latent \u001b[39m=\u001b[39m pipe(\n\u001b[1;32m     14\u001b[0m         prompt\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mprompt_1 \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mprompt_1 \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m         prompt_2\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mprompt_2 \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mprompt_2 \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         generator\u001b[39m=\u001b[39mgenerator,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m unrefined_image \u001b[39m=\u001b[39m postprocess_latent(latent)\n",
            "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mpostprocess_latent\u001b[0;34m(latent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpostprocess_latent\u001b[39m(latent):\n\u001b[0;32m----> 2\u001b[0m     vae_output \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mvae\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m      3\u001b[0m         latent\u001b[39m.\u001b[39;49mimages \u001b[39m/\u001b[39;49m pipe\u001b[39m.\u001b[39;49mvae\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mscaling_factor, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m     )[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m pipe\u001b[39m.\u001b[39mimage_processor\u001b[39m.\u001b[39mpostprocess(vae_output, output_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpil\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_hf_hook\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook, \u001b[39m\"\u001b[39m\u001b[39mpre_forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:270\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    268\u001b[0m     decoded \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(z)\u001b[39m.\u001b[39msample\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m (decoded,)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:257\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiled_decode(z, return_dict\u001b[39m=\u001b[39mreturn_dict)\n\u001b[1;32m    256\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 257\u001b[0m dec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(z)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m (dec,)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/diffusers/models/vae.py:270\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, z, latent_embeds)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[39m# up\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[39mfor\u001b[39;00m up_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_blocks:\n\u001b[0;32m--> 270\u001b[0m         sample \u001b[39m=\u001b[39m up_block(sample, latent_embeds)\n\u001b[1;32m    272\u001b[0m \u001b[39m# post-process\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m latent_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:2277\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, temb\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2276\u001b[0m     \u001b[39mfor\u001b[39;00m resnet \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnets:\n\u001b[0;32m-> 2277\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb\u001b[39m=\u001b[39;49mtemb)\n\u001b[1;32m   2279\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m         \u001b[39mfor\u001b[39;00m upsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers:\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/diffusers/models/resnet.py:625\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb)\u001b[0m\n\u001b[1;32m    623\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(hidden_states, temb)\n\u001b[1;32m    624\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(hidden_states)\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embedding_norm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mscale_shift\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    628\u001b[0m     scale, shift \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(temb, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/nn/modules/normalization.py:273\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 273\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgroup_norm(\n\u001b[1;32m    274\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_groups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/nn/functional.py:2530\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2529\u001b[0m _verify_batch_size([\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_groups, num_groups] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m:]))\n\u001b[0;32m-> 2530\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgroup_norm(\u001b[39minput\u001b[39;49m, num_groups, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 23.68 GiB total capacity; 6.27 GiB already allocated; 264.38 MiB free; 6.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "if config.use_ensemble_of_experts:\n",
        "    latent = pipe(\n",
        "        prompt=config.prompt_1 if config.prompt_1 != \"\" else None,\n",
        "        prompt_2=config.prompt_2 if config.prompt_2 != \"\" else None,\n",
        "        negative_prompt=config.negative_prompt_1 if config.negative_prompt_1 != \"\" else None,\n",
        "        negative_prompt_2=config.negative_prompt_2 if config.negative_prompt_2 != \"\" else None,\n",
        "        output_type=\"latent\",\n",
        "        num_inference_steps=config.num_inference_steps,\n",
        "        denoising_end=config.high_noise_fraction,\n",
        "        generator=generator,\n",
        "    )\n",
        "else:\n",
        "    latent = pipe(\n",
        "        prompt=config.prompt_1 if config.prompt_1 != \"\" else None,\n",
        "        prompt_2=config.prompt_2 if config.prompt_2 != \"\" else None,\n",
        "        negative_prompt=config.negative_prompt_1 if config.negative_prompt_1 != \"\" else None,\n",
        "        negative_prompt_2=config.negative_prompt_2 if config.negative_prompt_2 != \"\" else None,\n",
        "        output_type=\"latent\",\n",
        "        num_inference_steps=config.num_inference_steps,\n",
        "        generator=generator,\n",
        "    )\n",
        "unrefined_image = postprocess_latent(latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F25B6-n7wDC4"
      },
      "outputs": [],
      "source": [
        "# if config.use_ensemble_of_experts:\n",
        "#     refined_image = refiner(\n",
        "#         prompt=config.prompt_1 if config.prompt_1 != \"\" else None,\n",
        "#         prompt_2=config.prompt_2 if config.prompt_2 != \"\" else None,\n",
        "#         negative_prompt=config.negative_prompt_1 if config.negative_prompt_1 != \"\" else None,\n",
        "#         negative_prompt_2=config.negative_prompt_2 if config.negative_prompt_2 != \"\" else None,\n",
        "#         image=latent.images,\n",
        "#         num_inference_steps=config.num_refinement_steps,\n",
        "#         denoising_start=config.high_noise_fraction,\n",
        "#         generator=generator,\n",
        "#     ).images[0]\n",
        "# else:\n",
        "#     refined_image = refiner(\n",
        "#         prompt=config.prompt_1 if config.prompt_1 != \"\" else None,\n",
        "#         prompt_2=config.prompt_2 if config.prompt_2 != \"\" else None,\n",
        "#         negative_prompt=config.negative_prompt_1 if config.negative_prompt_1 != \"\" else None,\n",
        "#         negative_prompt_2=config.negative_prompt_2 if config.negative_prompt_2 != \"\" else None,\n",
        "#         image=latent.images[0][None, :],\n",
        "#         generator=generator,\n",
        "#     ).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unrefined_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPla-GIuwDC5"
      },
      "source": [
        "## Logging the Images to Weights & Biases\n",
        "\n",
        "Now, we log the images to Weights & Biases. This enables us to:\n",
        "\n",
        "- Visualize our generations\n",
        "- Examine the generated images across different images\n",
        "- Ensure reproducibility of the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxX73fAqwDC5"
      },
      "outputs": [],
      "source": [
        "# # Create a [wandb table](https://docs.wandb.ai/guides/tables)\n",
        "# table = wandb.Table(columns=[\n",
        "#     \"Prompt-1\",\n",
        "#     \"Prompt-2\",\n",
        "#     \"Negative-Prompt-1\",\n",
        "#     \"Negative-Prompt-2\",\n",
        "#     \"Unrefined-Image\",\n",
        "#     \"Refined-Image\",\n",
        "#     \"Use-Ensemble-of-Experts\",\n",
        "# ])\n",
        "\n",
        "# unrefined_image = wandb.Image(unrefined_image)\n",
        "# refined_image = unrefined_image\n",
        "# # wandb.Image(refined_image)\n",
        "\n",
        "# # Add the images to the table\n",
        "# table.add_data(\n",
        "#     config.prompt_1,\n",
        "#     config.prompt_2,\n",
        "#     config.negative_prompt_1,\n",
        "#     config.negative_prompt_2,\n",
        "#     unrefined_image,\n",
        "#     refined_image,\n",
        "#     config.use_ensemble_of_experts,\n",
        "# )\n",
        "\n",
        "# # Log the images and table to wandb\n",
        "# wandb.log({\n",
        "#     \"Unrefined-Image\": unrefined_image,\n",
        "#     \"Refined-Image\": refined_image,\n",
        "#     \"Text-to-Image\": table\n",
        "# })\n",
        "\n",
        "# # finish the experiment\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvD3SHZ1wDC5"
      },
      "source": [
        "Here's how you can examine your generations across multiple experiments ðŸ‘‡\n",
        "\n",
        "![](https://i.imgur.com/zNynGye.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagJo4rFwDC5"
      },
      "source": [
        "Here's how you can manage your prompts and your generations across experiments ðŸ‘‡\n",
        "\n",
        "![](https://i.imgur.com/JVEXkx0.png)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
